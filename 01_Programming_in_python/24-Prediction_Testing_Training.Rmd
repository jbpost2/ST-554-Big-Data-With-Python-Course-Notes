---
title: "Prediction and Training/Test Set Ideas"
author: "Justin Post"
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    css: [default, "css/ncsu.css", "css/ncsu-fonts.css", "css/mycss.css"]
    nature:
      beforeInit: ["js/ncsu-scale.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "partials/header.html"
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  fig.align = "center",
  #fig.width = 11,
  #fig.height = 5
  cache = FALSE
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(tidyverse)
library(knitr)
library(reticulate)
#use_python("C:\\Users\\jbpost2\\AppData\\Local\\Programs\\Python\\Python310\\python.exe")
#use_python("C:\\python\\python.exe")
use_python("C:\\ProgramData\\Anaconda3\\python.exe")
options(dplyr.print_min = 5)
options(reticulate.repl.quiet = TRUE)
```


layout: false
class: title-slide-section-red, middle


# Prediction and Training/Test Set Ideas
Justin Post

---
layout: true

<div class="my-footer"><img src="img/logo.png" style="height: 60px;"/></div> 


---

# Predictive Modeling Idea

- Choose form of model 

- Fit model to data using some algorithm 
    + Usually can be written as a problem where we minimize some loss function 
    
- Evaluate the model using a metric
    + RMSE very common for a numeric response

---

# Predictive Modeling Idea

- Choose form of model 

- Fit model to data using some algorithm 
    + Usually can be written as a problem where we minimize some loss function 

- Evaluate the model using a metric
    + RMSE very common for a numeric response

- Ideally we want our model to predict well for observations **it has yet to see**!


---

# Training vs Test Sets

- Evaluation of predictions over the observations used to *fit or train the model* is called the **training (set) error**
- Using RMSE as our metric: 

$$\mbox{Training RMSE} = \sqrt{\frac{1}{\mbox{# of obs used to fit model}}\sum_{\mbox{obs used to fit model}}(y-\hat{y})^2}$$

---

# Training vs Test Sets

- Evaluation of predictions over the observations used to *fit or train the model* is called the **training (set) error**
- Using RMSE as our metric: 

$$\mbox{Training RMSE} = \sqrt{\frac{1}{\mbox{# of obs used to fit model}}\sum_{\mbox{obs used to fit model}}(y-\hat{y})^2}$$

- If we only consider this, we'll have no idea how the model will fare on data it hasn't seen!


---

# Training vs Test Sets

One method is to split the data into a **training set** and **test set**
- On the training set we can fit (or train) our models
- We can then predict for the test set observations and judge effectiveness with our metric

```{r, echo = FALSE, out.width="600px"}
knitr::include_graphics("img/trainingtest.png")
```


---

# Example of Fitting and Evaluating Models

Consider our data set on motorcycle sale prices

```{python}
import pandas as pd
import numpy as np
bike_data = pd.read_csv("https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv")
bike_data['log_selling_price'] = np.log(bike_data['selling_price'])
bike_data['log_km_driven'] = np.log(bike_data['km_driven'])
print(bike_data.columns)
```


---

# Example of Fitting and Evaluating Models

- Response variable of `log_selling_price = ln(selling_price)`

- Consider three linear regression models:
$$\mbox{Model 1: log_selling_price = intercept + slope*year + Error}$$
$$\mbox{Model 2: log_selling_price = intercept + slope*log_km_driven + Error}$$
$$\mbox{Model 3: log_selling_price = intercept + slope*log_km_driven + slope*year + Error}$$
---

layout: false 

# Fitting the Models with `sklearn`

```{python, include = FALSE, eval = TRUE, echo = FALSE}
from sklearn import linear_model
reg1 = linear_model.LinearRegression() #Create a reg object
reg2 = linear_model.LinearRegression() #Create a reg object
reg3 = linear_model.LinearRegression() #Create a reg object
reg1.fit(bike_data['year'].values.reshape(-1,1), bike_data['log_selling_price']) 
reg2.fit(bike_data['log_km_driven'].values.reshape(-1,1), bike_data['log_selling_price']) 
reg3.fit(bike_data[['year', 'log_km_driven']], bike_data['log_selling_price']) 
```

```{python, eval = FALSE}
from sklearn import linear_model
reg1 = linear_model.LinearRegression() #Create a reg object
reg2 = linear_model.LinearRegression() #Create a reg object
reg3 = linear_model.LinearRegression() #Create a reg object
reg1.fit(bike_data['year'].values.reshape(-1,1), bike_data['log_selling_price']) 
reg2.fit(bike_data['log_km_driven'].values.reshape(-1,1), bike_data['log_selling_price']) 
reg3.fit(bike_data[['year', 'log_km_driven']], bike_data['log_selling_price']) 
```

```{python}
print(reg1.intercept_, reg1.coef_)
print(reg2.intercept_, reg2.coef_)
print(reg3.intercept_, reg3.coef_)
```


---

# Example of Fitting and Evaluating Models

- Now we have the fitted models.  Want to use them to predict the response
$$\mbox{Model 1: } \widehat{\mbox{log_selling_price}} = -201.06 + 0.105*\mbox{year}$$
$$\mbox{Model 2: } \widehat{\mbox{log_selling_price}} = 14.64 -0.391*\mbox{log_km_driven}$$

$$\mbox{Model 3: } \widehat{\mbox{log_selling_price}} = -148.79 + 0.080*\mbox{year}-0.227*\mbox{log_km_driven}$$

---

# Example of Fitting and Evaluating Models

- Now we have the fitted models.  Want to use them to predict the response
$$\mbox{Model 1: } \widehat{\mbox{log_selling_price}} = -201.06 + 0.105*\mbox{year}$$
$$\mbox{Model 2: } \widehat{\mbox{log_selling_price}} = 14.64 -0.391*\mbox{log_km_driven}$$

$$\mbox{Model 3: } \widehat{\mbox{log_selling_price}} = -148.79 + 0.080*\mbox{year}-0.227*\mbox{log_km_driven}$$

- Use the `.predict()` method
```{python}
pred1 = reg1.predict(bike_data['year'].values.reshape(-1,1))
pred2 = reg2.predict(bike_data['log_km_driven'].values.reshape(-1,1))
pred3 = reg3.predict(bike_data[['year', 'log_km_driven']])
pd.DataFrame(zip(pred1, pred2, pred3, bike_data['log_selling_price']), 
             columns = ["Model1", "Model2", "Model3", "Actual"])
```



---

# Example of Fitting and Evaluating Models

- Find **training** RMSE
```{python}
from sklearn.metrics import mean_squared_error
RMSE1 = np.sqrt(mean_squared_error(y_true = bike_data['log_selling_price'], y_pred = pred1))
RMSE2 = np.sqrt(mean_squared_error(bike_data['log_selling_price'], pred2))
RMSE3 = np.sqrt(mean_squared_error(bike_data['log_selling_price'], pred3))
print(round(RMSE1, 3), round(RMSE2, 3), round(RMSE3, 3))
```

- Estimate of RMSE is too **optimistic** compared to how the model would perform with new data!  Redo with train/test split!


---

# Train/Test Split

- `sklearn` has a function to make splitting data easy
- Commonly use 80/20 or 70/30 split

---

# Train/Test Split

- `sklearn` has a function to make splitting data easy
- Commonly use 80/20 or 70/30 split

```{python}
from sklearn.model_selection import train_test_split
#Function will return a list with four things:
#Test/train for predictors (X)
#Test/train for response (y)
X_train, X_test, y_train, y_test = train_test_split(
  bike_data[["year", "log_km_driven"]],
  bike_data["log_selling_price"], 
  test_size=0.20, 
  random_state=422)
```



---

# Fit or Train Model

- We then fit the model on the training set

```{python, eval = FALSE}
reg1 = linear_model.LinearRegression() #Create a reg object
reg2 = linear_model.LinearRegression() #Create a reg object
reg3 = linear_model.LinearRegression() #Create a reg object
reg1.fit(X_train['year'].values.reshape(-1,1), y_train.values) 
reg2.fit(X_train['log_km_driven'].values.reshape(-1,1), y_train.values) 
reg3.fit(X_train[['year', 'log_km_driven']], y_train.values) 
```


```{python, include = FALSE, eval = TRUE, echo = FALSE}
reg1 = linear_model.LinearRegression() #Create a reg object
reg2 = linear_model.LinearRegression() #Create a reg object
reg3 = linear_model.LinearRegression() #Create a reg object
reg1.fit(X_train['year'].values.reshape(-1,1), y_train.values) 
reg2.fit(X_train['log_km_driven'].values.reshape(-1,1), y_train.values) 
reg3.fit(X_train[['year', 'log_km_driven']], y_train.values) 
```

---

# Fit or Train Model

- We then fit the model on the training set

```{python, eval = FALSE}
reg1 = linear_model.LinearRegression() #Create a reg object
reg2 = linear_model.LinearRegression() #Create a reg object
reg3 = linear_model.LinearRegression() #Create a reg object
reg1.fit(X_train['year'].values.reshape(-1,1), y_train.values) 
reg2.fit(X_train['log_km_driven'].values.reshape(-1,1), y_train.values) 
reg3.fit(X_train[['year', 'log_km_driven']], y_train.values) 
```

- Can look at training RMSE if we want

```{python}
train_RMSE1 = np.sqrt(mean_squared_error(y_train.values, 
                                         reg1.predict(X_train['year'].values.reshape(-1,1))))
train_RMSE2 = np.sqrt(mean_squared_error(y_train.values, 
                                         reg2.predict(X_train['log_km_driven'].values.reshape(-1,1))))
train_RMSE3 = np.sqrt(mean_squared_error(y_train.values, 
                                         reg3.predict(X_train[['year', 'log_km_driven']])))
print(round(train_RMSE1, 3), round(train_RMSE2, 3), round(train_RMSE3, 3))
```



---

# Test Error

- Now we look at predictions on the test set
    + Test data **not** used when training model

```{python}
test_RMSE1 = np.sqrt(mean_squared_error(y_test.values, 
                                         reg1.predict(X_test['year'].values.reshape(-1,1))))
test_RMSE2 = np.sqrt(mean_squared_error(y_test.values, 
                                         reg2.predict(X_test['log_km_driven'].values.reshape(-1,1))))
test_RMSE3 = np.sqrt(mean_squared_error(y_test.values, 
                                         reg3.predict(X_test[['year', 'log_km_driven']])))
print(round(test_RMSE1, 3), round(test_RMSE2, 3), round(test_RMSE3, 3))
```

- When choosing a model, if the RMSE values were 'close', we'd want to consider the interpretability of the model (and perhaps the assumptions if we wanted to do inference too!)


---

# Recap

- Choose form of model 
- Fit model to data using some algorithm 
    + Usually can be written as a problem where we minimize some loss function 
- Evaluate the model using a metric
    + RMSE very common for a numeric response

- Ideally we want our model to predict well for observations **it has yet to see**!
    

